<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
        "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/"/>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
    <link rel="stylesheet" href="css/main.css" type="text/css"/>

    <link rel="shortcut icon" href="img/favicon.png"/>
    <link rel="bookmark" href="img/favicon.png" type="image/x-icon" 　/>
    <title>欢迎来到槐硕的个人主页</title>
</head>
<body>
<!--<div class="aaamenu">-->
<!--<td>-->
<!--<a href="#aboutme">About me</a>-->
<!--<a href="https://scholar.google.com/citations?user=HWwI12YAAAAJ&hl=en" style="margin-left: 10px">Google Scholar</a>-->
<!--<a href="https://github.com/SHUO-HUAI" style="margin-left: 10px">GitHub</a>-->
<!--</td>-->
<!--</div>-->
<table summary="Table for page layout." id="tlayout">
    <tr valign="top">
        <td id="layout-content">

            <div id="toptitle" style="position: fixed; background-image: url('./img/background.png');
             background-size: cover; background-repeat: no-repeat;">
                <div style="display: inline">
                    <h1 style="display: inline; "> 槐硕 (Huai, Shuo） </h1></div>
                <div class="aaamenu" style="display: inline"><a href="#aboutme">关于我</a>
                    <a href="#publications" style="margin-left: 10px; ">发表刊物</a>
                    <a href="#projects" style="margin-left: 10px">项目</a>
                    <a href="#experience" style="margin-left: 10px">经历</a>
                    <a href="doc/cv.html" style="margin-left: 10px">简历</a>
                    <a href="blog.html" style="margin-left: 10px">博客</a>
                    <a href="index.html" style="margin-left: 10px">English/中文</a>
                </div>


            </div>
            <div style="position: fixed; top: 2.5em ;background: #ffffff; width: 100%;	z-index: 9998;">
                <br/> <br/> <br/> <br/>
            </div>

            <div style="position: absolute; margin-top: 6.8em; width: 98%">
                <table class="imgtable">
                    <tr>
                        <td>
                            <a href="img/bio.png"> <img src="img/bio.png" alt="alt text" width="131px" height="174px"/>&nbsp;&nbsp;&nbsp;
                            </a>
                        </td>
                        <td align="left"><p>博士，<br/>
                            计算机科学与工程学院，<br/>
                            南洋理工大学。 <br/>
                            南洋大道 50， 639798， 新加坡。 <br/><br/>
                            电子邮箱: <a href="mailto:shuohuai@outlook.com">shuohuai [AT] outlook [dot] com</a> <br/>
                            GitHub: <a href="https://github.com/SHUO-HUAI">https://github.com/shuo-huai</a> <br/>
                            谷歌学术: <a href="https://scholar.google.com/citations?user=HWwI12YAAAAJ">https://scholar.google.com/citations?user=HWwI12YAAAAJ</a>
                            <!--<br />Linkedin: <a href="https://www.linkedin.com/in/shuo-huai-7351001a0/">https://www.linkedin.com/in/shuo-huai-7351001a0</a>-->
                        </p>
                        </td>
                    </tr>
                </table>
                <h2>关于我</h2>
                <a class="anchorzh" id="aboutme"></a>
                <p>
                    我于2019年在山东大学计算机科学与技术学院获得学士学位，而目前正在新加坡南洋理工大学计算机科学与工程学院攻读博士学位。我的研究兴趣包括嵌入式智能、神经网络优化、内存计算和非易失性存储。</p>
                <!--<h2>Research and Publications</h2>-->
                <h3>研究兴趣</h3>
                <ul>
                    <li><p>嵌入式智能</p>
                    </li>
                    <li><p>神经网络优化</p>
                    </li>
                    <li><p>内存计算</p>
                    </li>
                    <li><p>非易失性存储</p>
                    </li>
                </ul>
                <h2>教育经历</h2>
                <h3>2020年01月—至今 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a
                        href="https://www.ntu.edu.sg/">南洋理工大学</a></h3>
                <!--                <div style="font-size: 40%">&nbsp;</div>-->
                <div style="font-size: 100%"><b>导师: <a href="https://personal.ntu.edu.sg/liu/"> 刘韦辰 </a></b>
                </div>
                <div style="font-size: 50%">&nbsp;</div>
                计算机科学与工程博士
                <!--                            <div style="font-size: 50%">&nbsp;</div>-->
                <!--                <ul>-->
                <!--                    <li><p>Main Courses: Machine Learning, Data Mining, Recommender System.</p>-->
                <!--                    </li>-->
                <!--                </ul>-->
                <h3>2015年09月—2019年06月 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a
                        href="https://www.sdu.edu.cn/">山东大学</a></h3>
                <div style="font-size: 100%"><b>导师: <a href="https://zmyhomepage.github.io/zmy_EN/"> 赵梦莹 </a></b>
                </div>
                <div style="font-size: 50%">&nbsp;</div>
                计算机科学与技术学士
                <h2>发表刊物 </h2>
                <a class="anchorzh" id="publications"></a>
                <h3>会议</h3>
                <ol>
                    <li><p>Chen, Hui; Liu, Di; Li, Shiqing; <b>Huai, Shuo</b>; Luo, Xiangzhong; Liu, Weichen; <a
                            href="https://dl.acm.org/doi/abs/10.1145/3566097.3567846">"MUGNoC: A
                        Software-Configured Multicast-Unicast-Gather NoC for Accelerating CNN Dataflows"</a>,
                        Proceedings of
                        the 28th Asia and South Pacific Design Automation Conference, 308-313, 2023.</p></li>
                    <li><p><b>Huai, Shuo</b>; Liu, Di; Luo, Xiangzhong; Chen, Hui; Liu, Weichen; Subramaniam, Ravi;
                        <a href="https://dl.acm.org/doi/abs/10.1145/3566097.3567856">"Crossbar-Aligned & Integer-Only
                            Neural Network Compression for Efficient in-Memory
                            Acceleration"</a>, Proceedings of the 28th Asia and South Pacific Design Automation
                        Conference,
                        234-239, 2023.</p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; Kong, Hao; <b>Huai, Shuo</b>; Chen, Hui; Liu, Weichen; <a
                            href="https://dl.acm.org/doi/abs/10.1145/3489517.3530488">"You only
                        search once: on lightweight differentiable architecture search for resource-constrained embedded
                        platforms"</a>, Proceedings of the 59th ACM/IEEE Design Automation Conference, 475-480, 2022.
                    </p>
                    </li>
                    <li><p><b>Huai, Shuo</b>; Liu, Di; Kong, Hao; Luo, Xiangzhong; Liu, Weichen; Subramaniam, Ravi;
                        Makaya, Christian; Lin, Qian; <a href="https://ieeexplore.ieee.org/abstract/document/9978336">"Collate:
                            Collaborative Neural Network Learning for
                            Latency-Critical Edge Systems"</a>, 2022 IEEE 40th International Conference on Computer
                        Design
                        (ICCD), 627-634, 2022, IEEE.</p></li>
                    <li><p>Kong, Hao; Liu, Di; <b>Huai, Shuo</b>; Luo, Xiangzhong; Liu, Weichen; Subramaniam, Ravi;
                        Makaya, Christian; Lin, Qian; <a href="https://dl.acm.org/doi/abs/10.1145/3508352.3549397">"Smart
                            Scissor: Coupling Spatial Redundancy Reduction and CNN
                            Compression for Embedded Hardware"</a>, Proceedings of the 41st IEEE/ACM International
                        Conference on
                        Computer-Aided Design, 9-Jan, 2022.</p></li>
                    <li><p><b>Huai, Shuo</b>; Zhang, Lei; Liu, Di; Liu, Weichen; Subramaniam, Ravi; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9586309">"ZeroBN: Learning
                        Compact Neural Networks For Latency-Critical Edge Systems"</a>, 2021 58th ACM/IEEE Design
                        Automation
                        Conference (DAC),151-156, 2021, IEEE.</p></li>
                    <li><p><b>Huai, Shuo</b>; Song, Weining; Zhao, Mengying; Cai, Xiaojun; Jia, Zhiping; <a
                            href="https://dl.acm.org/doi/abs/10.1145/3316781.3317881">
                        "Performance-aware wear leveling for block ram in nonvolatile fpgas"</a>, Proceedings of the
                        56th
                        Annual Design Automation Conference 2019, pp. 1-6, 2019.</p></li>
                </ol>
                <h3>期刊</h3>
                <ol>
                    <li><p>Luo, Xiangzhong; Liu, Di; Kong, Hao; <b>Huai, Shuo</b>; Chen, Hui; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9817049"> "SurgeNAS: A
                        Comprehensive Surgery on Hardware-Aware Differentiable Neural Architecture Search"</a>, IEEE
                        Transactions on Computers, 2022, IEEE.</p></li>
                    <li><p>Chen, Hui; Chen, Peng; Luo, Xiangzhong; <b>Huai, Shuo</b>; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9709895">"LAMP: Load-bAlanced
                        Multipath Parallel Transmission in Point-to-point NoCs"</a>, IEEE Transactions on Computer-Aided
                        Design of Integrated Circuits and Systems, 2022, IEEE.</p></li>
                    <li><p><b>Huai, Shuo</b>; Liu, Di; Kong, Hao; Liu, Weichen; Subramaniam, Ravi; Makaya, Christian;
                        Lin, Qian; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167739X22004253">
                            "Latency-constrained DNN architecture learning for edge systems using zerorized batch
                            normalization"</a>, Future Generation Computer Systems, 2022, Elsevier.</p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; Kong, Hao; <b>Huai, Shuo</b>; Chen, Hui; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/abstract/document/9896156">"LightNAS:
                        On Lightweight and Scalable Neural Architecture Search for Embedded Platforms"</a>, IEEE
                        Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022, IEEE.</p></li>
                    <li><p>Luo, Xiangzhong; Liu, Di; <b>Huai, Shuo</b>; Kong, Hao; Chen, Hui; Liu, Weichen; <a
                            href="https://ieeexplore.ieee.org/document/9496596"> "Designing
                        Efficient DNNs via Hardware-Aware Neural Architecture Search and Beyond"</a>, IEEE Transactions
                        on
                        Computer-Aided Design of Integrated Circuits and Systems, 41, 6, 1799-1812, 2021, IEEE.</p></li>
                    <li><p>Kong, Hao; <b>Huai, Shuo</b>; Liu, Di; Zhang, Lei; Chen, Hui; Zhu, Shien; Li, Shiqing; Liu,
                        Weichen; Rastogi, Manu; Subramaniam, Ravi; <a
                                href="https://ieeexplore.ieee.org/document/9475509">"EDLAB: A benchmark for edge deep
                            learning
                            accelerators"</a>, IEEE Design & Test, 39, 3, 17-Aug, 2021, IEEE.</p></li>
                </ol>
                <!--<p><b>Note</b>: * indicates the corresponding author.</p>-->
                <p><a href="https://scholar.google.com/citations?user=HWwI12YAAAAJ&hl=en">完整列表参见谷歌学术。</a></p>
                <h2>发明</h2>
                <h3>专利</h3>
                <ol>
                    <li>
                        <p>题目： 一种基于性能磨损均衡的非易失FPGA布局优化方法和系统<br/>
                            发明人： 1） 赵梦莹; 2） <b>槐硕</b>; 3） 申兆岩; 4） 蔡晓军; 5） 贾智平<br/>
                            申请日： 2019年05月20日<br/>
                            专利号： ZL 2019 1 0419760.9
                        </p>
                    </li>
                </ol>
                <h3>技术公开</h3>
                <ol>
                    <li><p>题目: EDLAB: A Benchmark Tool for Edge Deep Learning Accelerators <br/>
                        发明人: 1) Liu Weichen; 2) Liu Di; 3) Kong Hao; 4) Zhang Lei; 5) <b>Huai Shuo</b>; 6) Li
                        Shiqing; 7) Chen Hui; 8) Zhu Shien <br/>
                        申请日: 2020年07月08日 <br/>
                        序号: TD 2020-264 <br/>
                    </p>
                    </li>
                    <li><p>Title: ZeroBN: Learning Compact Neural Networks For Latency-Critical Edge Systems <br/>
                        Inventors: 1) Liu Weichen; 2) <b>Huai Shuo</b>; 3) Liu Di; 4) Zhang Lei <br/>
                        Filed date: 09 March 2021 <br/>
                        Ref: TD 2021-096 <br/>
                    </p>
                    </li>
                    <li><p>Title: Collaborative Neural Network Learning For Multiple Latency-Critical Edge Systems <br/>
                        Inventors: 1) Liu Weichen; 2) <b>Huai Shuo</b>; 3) Hao Kong <br/>
                        Filed date: 15 August 2022 <br/>
                        Ref: TD 2022-287 <br/>
                    </p>
                    </li>
                    <li><p>Title: Smart Scissor: A Deep Compression Framework For Jointly Reducing the Redundancy In
                        Images And Neural Networks <br/>
                        Inventors: 1) Liu Weichen; 2) Hao Kong; 3) <b>Huai Shuo</b> <br/>
                        Filed date: 15 August 2022 <br/>
                        Ref: TD 2022-288 <br/>
                    </p>
                    </li>
                </ol>
                <h2>Projects</h2>
                <a class="anchor" id="projects"></a>
                <ol>
                    <li><p><a href="p1.html"> Learning Compact DNNs for Latency-Critical Edge Systems</a></p></li>
                    &nbsp;&nbsp;— Learn compact DNNs under a ‘hard’ latency constraint upon targeted hardware by one
                    training process.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>We propose a novel compact model training method ZeroBN that
                            can extract the optimal architecture to satisfy a ‘hard’ latency
                            constraint by only one training process;</p>
                        </li>
                        <li><p>We propose a scheme applying the global importance rank of
                            channels and constructing a dynamic Zero-Recovery training
                            process. It extends the exploration space of ZeroBN to learn
                            the optimal architecture.</p>
                        </li>
                        <li><p>We propose a machine learning-based latency predictor, which
                            is embedded into ZeroBN to provide a compression ratio for
                            each Zero phase.</p>
                        </li>
                        <li><p>We demonstrate the efficiency and effectiveness of our ZeroBN
                            with extensive experiments. Without the need to perform refinement, our method does not
                            cause a large accuracy drop
                            compared to the original model. On CIFAR-10 dataset, we
                            even improve the accuracy of all models by 0.24% to 0.32%.
                            On ImageNet-100 dataset, under a latency constraint, we
                            improve 0.3% accuracy for GoogLeNet.</p></li>
                    </ul>
                    <li><p><a href="p2.html">Collaborative DNNs Learning for Multiple Latency-Critical Edge Systems </a>
                    </p></li>
                    &nbsp;&nbsp;— Optimize federated learning algorithm to simultaneously meet the latency constraints
                    of all participating systems while obtaining high accuracy.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>We propose a novel model learning framework, Collate, that
                            cultivates optimal DNN architectures collaboratively for multiple
                            edge systems to obtain higher accuracy and satisfy their latency
                            constraints with only one training process.</p>
                        </li>
                        <li><p>We present a proto-corrected aggregation scheme in the global
                            training process to effectively aggregate all heterogeneous models from each edge system for
                            higher accuracy.</p>
                        </li>
                        <li><p>We demonstrate the effectiveness of Collate with extensive
                            experiments. Compared to the state-of-the-art methods and under
                            the same latency constraints, our extended models can improve
                            the accuracy by 1.96% on average, and the accuracy of shrunk
                            models outperforms others by 3.09% on average.</p></li>
                    </ul>
                    <li><p><a href="p3.html">In-Memory Computing Based Neural Network Compression</a></p></li>
                    &nbsp;&nbsp;— Apply crossbar-aligned pruning and integer-only quantization to completely achieve
                    efficient and low-power IMC acceleration.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>We introduce a crossbar-aligned pruning approach to reduce crossbar usage without extra
                            processing units for better hardware efficiency and greater integration density. Also, it
                            includes both kernel-group pruning and crossbar pruning to form multi-grained pruning for
                            high accuracy and large sparsity. </p>
                        </li>
                        <li><p>We apply a simple yet efficient integer-only quantization scheme for IMC architecture by
                            reusing the bit-shift units. The quantization approach is co-optimized with the pruning
                            strategy during the training process to improve accuracy.</p>
                        </li>
                        <li><p>We demonstrate the efficiency and effectiveness of our framework with extensive
                            experiments. Compared to state-of-the-art methods, our method can achieve a higher sparsity
                            rate and a slighter accuracy drop without extra hardware. Thus, we can reduce computing
                            power and computing area significantly.</p>
                        </li>
                    </ul>
                    <li><p><a href="p4.html">Performance-aware Wear Leveling Placement Algorithm for Nonvolatile
                        FPGAs</a></p></li>
                    &nbsp;&nbsp;— A performance-aware wear leveling
                    strategy for BRAM blocks in non-volatile FPGAs to improve the
                    lifetime, while maintaining high efficiency.
                    <div style="font-size: 50%">&nbsp;</div>
                    <ul>
                        <li><p>Propose to simultaneously consider lifetime and performance in WL for BRAM blocks in FPGA
                            systems.</p>
                        </li>
                        <li><p>Design a performance-aware WL mechanism to balance
                            write traffic between BRAM blocks. The placement strategy is improved by integrating write
                            information so that
                            multiple configuration files can be generated for dynamic
                            switching.</p>
                        </li>
                        <li><p>Develop a simulator to evaluate performance and lifetime
                            for non-volatile FPGAs. The evaluation results show significant lifetime improvement with
                            little performance overhead.</p>
                        </li>
                    </ul>

                </ol>


                <h2>Experience</h2>
                <a class="anchor" id="experience"></a>
                <h3>Work</h3>
                <ol>
                    <li><p>Project Officer, <a href="https://www.ntu.edu.sg/hp-ntu-corp-lab">HP-NTU Digital
                        Manufacturing Corporate Lab</a>, September 2019 - October 2023</p></li>
                    &nbsp;&nbsp;— Machine Learning Optimization for Edge Devices
                    <ul>
                        <li><p>Design a performance evaluation methodology and toolset for Machine Learning Accelerators
                            (MLAs).</p>
                        </li>
                        <li><p> Provide HP business groups, like Business Personal Systems and Digital Manufacturing,
                            the insights and framework for process and resource efficient design and optimizing of DNN,
                            accelerators and system architectures for machine learning/inference at the edge.</p></li>
                    </ul>
                </ol>

                <h3>Teaching</h3>
                <ol>
                    <li><p>Teaching Assistant, <a
                        href="https://www.ntu.edu.sg/">Nanyang Technological University</a> , February 2020 - March 2020 </p></li>
                    <ul>
                        <li><p>CE/CZ 1006 Computer Organisation & Architecture</p>
                        </li>
                    </ul>
                        <li><p>Teaching Assistant, <a
                        href="https://www.sdu.edu.cn/">Shandong University</a> , February 2019 - June 2019 </p></li>
                    <ul>
                        <li><p>SD01331470 Computer Organization and Design</p>
                        </li>
                    </ul>
                </ol>
                <div style="font-size:100%">&nbsp;</div>
                <div>
                    <hr/>
                </div>
                <div align="center" style=" width: 100%" class="cropped "><img
                        src="https://s11.flagcounter.com/count/Bj2R/bg_FFFFFF/txt_000000/border_ffffff/columns_8/maxflags_8/viewers_3/labels_1/pageviews_0/flags_0/percent_0"
                        alt="Flag Counter"></div>
                <div style="font-size:50%">&nbsp;</div>
                <div style="height: 38px; width: 0.1%;  background-color: rgba(198,198,198,0.6); text-align: center; font-size: 90%; display: table-cell; vertical-align: middle; table-layout: auto;  ">
                    Copyright &copy; 2022-
                    <script>document.write(new Date().getFullYear())</script>
                    Huai Shuo | Last Update: 20 February 2023
                </div>
            </div>
        </td>
    </tr>
</table>
</body>
</html>